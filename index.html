<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>COWP Project</title>

    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link href="/css/css" rel="stylesheet" type="text/css">

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.998.0" data-gr-ext-installed="">
    <link media="all" href="/css/glab.css" type="text/css" rel="StyleSheet">

    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }

        #primarycontent {
            MARGIN-LEFT: auto;
            WIDTH: expression(document.body.clientWidth > 500? "500px": "auto");
            MARGIN-RIGHT: auto;
            TEXT-ALIGN: left;
            max-width: 850px;
        }

        BODY {
            TEXT-ALIGN: center
        }

    </style>

    <div id="primarycontent">
        <center>
            <h0>Integrating Action Knowledge and LLMs for Task Planning and Situation Handling </br> in Open Worlds</h0>
        </center>
        <center>
            <p>Yan Ding<sup>1</sup>,&nbsp;Xiaohan Zhang<sup>1</sup>,&nbsp;Saeid Amiri<sup>1</sup>,&nbsp;Nieqing Cao<sup>1</sup>,&nbsp;</br>Hao Yang<sup>2</sup>,&nbsp;Andy Kaminski<sup>2</sup>,&nbsp;Chad Esselink<sup>2</sup>,&nbsp;Shiqi Zhang<sup>1</sup></p>
            <center>
                <p><sup>1</sup>SUNY Binghamton &nbsp;<sup>2</sup>Ford Motor Company</p>
                <center>
                    <!--                    <p>IEEE RA-L, 2022 </p>-->
                    <p>[<a href="https://github.com/yding25/GPT-Planner">Code</a>]&nbsp[<a href="files/COWP_dataset_6tasks.xlsx">Situation Dataset</a>]
                    </p>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td valign="middle" align="center">
                                    <iframe width="600" height="400" src="https://www.youtube.com/embed/pHSxx9AqwC4" frameborder="0" allowfullscreen></iframe>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <h1 align="center">Abstract</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%">Task planning systems have been developed to help robots use human knowledge (about actions) to
                            complete long-horizon tasks. Most of them have been developed for “closed worlds” while assuming
                            the robot is provided with complete world knowledge. However, the real world is generally open,
                            and the robots frequently encounter unforeseen situations that can potentially break the planner’s
                            completeness. Could we leverage the recent advances on pre-trained Large Language Models (<b>LLMs</b>)
                            to enable <b>classical planning systems</b> to deal with novel situations? </p>

                        <p align="justify" width="20%">This paper introduces a novel framework, called <b>COWP</b>, for open-world task planning and situation
                            handling. COWP dynamically augments the robot’s action knowledge, including the preconditions
                            and effects of actions, with task-oriented commonsense knowledge. COWP embraces the openness
                            from LLMs, and is grounded to specific domains via action knowledge. For systematic evaluations,
                            we collected a dataset that includes 1,085 execution-time situations. Each situation corresponds to a
                            state instance wherein a robot is potentially unable to complete a task using a solution that normally
                            works. Experimental results show that our approach outperforms competitive baselines from the
                            literature in the success rate of service tasks. Additionally, we have demonstrated COWP using a
                            mobile manipulator.
                        </p>
                    </div>

                    <br>

                    <h1 align="center">Framework</h1>
                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/fig_cowp.png" style="width:100%;margin-left:0%;margin-right:0%;">
                        </td>
                    </table>

                    <div style="font-size:30px">
                        <p align="justify" width="20%">
                            An overview of COWP that includes the three key components of <b>Task Planner</b> (provided as prior knowledge under closed-world assumption), <b>Knowledge Acquirer</b>, and <b>Plan Monitor</b>.
                        </p>
                        <ul>
                            <li>
                                <p align="justify" width="20%">
                                    The <b style="color:#008000">green</b> (dashed) loop represents a plan execution process where the robot does encounter no situation, or these situations have no impact on the robot's plan execution.
                                </p>
                            </li>
                            <li line-height="10px">
                                <p align="justify" width="20%">The <b style="color:#FFA500">orange</b> loop is activated when the robot's current (closed-world) task planner is unable to develop a plan, which activates Knowledge Acquirer to augment the task planner with additional action effects utilizing common sense.
                                </p>
                            </li>
                        </ul>
                    </div>

                    <br>

                    <h1 align="center">Algorithm</h1>
                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/algorithm.png" style="width:100%;margin-left:0%;margin-right:0%;">
                        </td>
                    </table>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> Algorithm 1 describes how the components of COWP interact with each other. More details can be found in the paper.</p>
                    </div>

                    <br>

                    <h1 align="center">Closed-World Task Planners in PDDL</h1>

                    <div style="font-size:30px">
                        <p align="justify" width="20%"> For each task in the evaluation, we developed a closed-world task planner in PDDL [1]. PDDL, an action-centered language, is designed to formalize Artificial Intelligence (AI) planning problems, allowing for a more direct comparison of planning algorithms and implementations.</p>

                        <p align="justify" width="20%"> The below figure shows a task planner for the task of ``serving water'', which consists of a domain file (<b>upper</b>) and a problem file (<b>lower</b>).</p>

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_planner.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>

                        <p align="justify" width="20%"> In the upper subfigure, a set of predicates (e.g., <b>cup_at</b>) and a set of actions (e.g., <b>fill</b>) are predefined, where an action is defined by its preconditions and effects. For example, one of preconditions for action <b>fill</b> is <b>(cup_is_held ?c) - (cup_is_empty ?c)</b>, and the action effect is <b>(cup_is_filled ?c)</b>.</p>

                        <p align="justify" width="20%"> In the lower subfigure, a task problem is defined by an initial state and a goal state (i.e., a user is satisfied and the faucet is turned off.) A task plan for drinking water is generated after inputting these two files into a solver, as shown below: </p>
                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/task_plan.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>

                        <p align="justify" width="20%"> The solver is accessible at <a href="http://editor.planning.domains">http://editor.planning.domains/</a>.</p>
                    </div>

                    <br>

                    <h1 align="center">Prompt Design</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> The realization of our plan monitor relies on repeatedly querying GPT-3 for each action using the following prompt.</p>
                        <p align="justify" width="20%"><b>Prompt 1</b>: <em>Is it suitable for a robot to [Perform-Action], if [Situation]?</em></p>

                        <p align="justify" width="20%">The following template is for querying an LLM for acquiring common sense about action effects.</p>

                        <p align="justify" width="20%"><b>Prompt 2</b>: <em> Is it suitable for a robot to [Perform-Action-with-Object]?</em></p>

                        <p align="justify" width="20%"><b>Prompt 3</b>: <em> There are some objects, such as [Object-1], [Object-2], ..., and [Object-N]. Which is the most suitable for [Current-Task], if [Situation]?</em></p>

                        <p align="justify" width="20%">The below figure shows three examples for prompt construction based on our <b>Templates 1-3</b>, respectively. In the figure, the interface, called Playground, is intended for testing GPT-3 online, where a user can text a prompt in the blank, and customize the hyperparameters of GPT-3 (e.g., model). In our case, we use the <b>text-davinci-003</b> model, which is the most capable engine.</p>

                        <p align="justify" width="20%">The prompt for Plan Monitor (PM) is constructed based on Template 1, where PM evaluates if the current task plan is feasible or not. In the <b>top</b> figure, we can know an action precondition that ``one cannot fill a broken cup with water'' according to the common sense from GPT-3. </p>

                        <p align="justify" width="20%">The prompts for Knowledge Acquirer (KA) are constructed based on Template 2 and Template 3, where KA extracts common sense to augment the classical task planner. In these two figures (<b>middle</b> and <b>bottom</b>), we can know that common sense that ``one can use a bowl for drinking water'' can be added into an action effect, according to the common sense from GPT-3.</p>

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_templates.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>
                        <p align="justify" width="20%">The playground of GPT-3 is accessible at <a href="https://beta.openai.com/playground">https://beta.openai.com/playground</a>.</p>.
                    </div>

                    <br>

                    <h1 align="center">Experiment Results</h1>
                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/Fig8.jpg" style="width:70%;margin-left:0%;margin-right:15%;">
                        </td>
                    </table>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/Fig9.jpg" style="width:70%;margin-left:0%;margin-right:15%;">
                        </td>
                    </table>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/Fig10.jpg" style="width:100%;margin-left:0%;margin-right:0%;">
                        </td>
                    </table>

                    <div style="font-size:30px">
                        <p align="justify" width="20%">
                            <b>Top:</b> The task completion percentage of COWP (ours) and four baseline methods under 12 different tasks. The x-
                            axis represents the task name, and the y-axis represents the task completion percentage. The task completion percentage
                            for each value is an average of 150 trials. The tasks are sorted based on the performance of COWP, where the very left
                            corresponds to its best performance.
                        </p>
                        <p align="justify" width="20%">
                            <b>Middle:</b> The situation handling percentage of COWP (ours) and three baseline methods under 12 different tasks, where
                            the x-axis represents the task name, and the y-axis represents the situation handling percentage. Each y value represents a
                            ratio of the number of handled situations to the total number of situations. The tasks are ranked based on the performance
                            of COWP, where the very left corresponds to its best performance.
                        </p>
                        <p align="justify" width="20%">
                            <b>Bottom:</b> The situation handling percentages of COWP (ours) and three baseline methods under different objects, where
                            the x-axis represents the object involved in the sampled situation, the number (X) beside each object is the occurrence of
                            the object in situations, and the y-axis represents the percentage of situation handling. The objects are ranked based on
                            the performance of COWP. In this analysis, we only display objects with an occurrence of more than 5.
                        </p>
                    </div>


                    <br>

                    <h1 align="center">Questionnaire for Collecting Situation Dataset</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> To collect execution-time situations, a questionnaire was designed and published on Amazon Mechanical Turk. The below figure shows the Mechanical Turk interface for one everyday task (i.e., serving water). In the interface, each MTurker was provided with a task description, including steps for completing the task. The MTurkers were asked to respond to a questionnaire by identifying one step in the provided plan and describing a situation that might occur in that step within the blank. </p>

                        <p align="justify" width="20%">On the questionnaire, there are six everyday tasks (e.g., setting a dining table) associated with their steps, which were extracted from an existing dataset [2]. In the end, we have collected a dataset of 561 valid situations, where each instance of the dataset corresponds to a situation that prevents a service robot from completing a task in a dining domain. In the next section, we will discuss the statistics of the dataset.</p>

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_questionnaire.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>
                    </div>

                    <br>

                    <h1 align="center">Statistics of Situation Dataset</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> The following two figures show the statistics of situations for six everyday tasks used in our evaluation, where x-axis reflects the occurrence of each distinguishable situations, and y-axis represents each distinguishable situations, respectively. In the top left corner of each subfigure, (X) represents the number of distinguishable situations in each task. In the bottom right corner of each subfigure, Total = X represents the number of situations in each task. According to the two figures, we can see that there are at least 92 situations collected for each of the six tasks used in our evaluation, with 16 to 22 distinguishable situations. </p>

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_situation123.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>
                        <!--                        <p align="justify" width="20%"><b>Top</b>: Details of situations in the task of “setting table”; <b>Middle</b>: Details of situations in the task of “drinking coke”; <b>Bottom</b>: Details of situations in the task of “preparing burger”; x-axis reflects the occurrence of each distinguishable situations, and y-axis represents each distinguishable situations, respectively. (X) in the top left corner of each subfigure represents the number of distinguishable situations in each task. Total = X indicates the number of situations in each task.</p>-->

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_situation456.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>
                        <!--                        <p align="justify" width="20%"><b>Top</b>: Details of situations in the task of “drinking water”; <b>Middle</b>: Details of situations in the task of “cleaning floor”; <b>Bottom</b>: Details of situations in the task of “washing plate”; x-axis reflects the occurrence of each distinguishable situations, and y-axis represents each distinguishable situations, respectively. (X) in the top left corner of each subfigure represents the number of distinguishable situations in each task. Total = X indicates the number of situations in each task.</p>-->
                    </div>

                    <br>

                    <h1 align="center">Object Library for Simulation</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> For simulating dining tasks, we extracted 86 objects (e.g., cup, burger, folk, table, and chair) from an existing dataset [2]. Fig. 4 shows these objects, which is categorized into five groups: utensil, appliance, furniture, food, and beverage. From the figure, we can see that the category “utensil” contains the greatest number of objects (i.e., 29), while the category “beverage” contains the fewest ones (i.e., 8). </p>

                        <table border="0" cellspacing="10" cellpadding="0" align="center">
                            <td>
                                <img src="./img/fig_object.png" style="width:100%;margin-left:0%;margin-right:0%;">
                            </td>
                        </table>
                        <!--                        <p align="justify" width="20%">For simulating dining tasks, we extracted 86 objects from an existing dataset. These objects are categorized into five groups: utensil, appliance, furniture, food, and beverage, with (X) representing the number of objects in each group.</p>-->
                    </div>

                    <h1 align="center">Reference</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%">[1] C. Aeronautiques, A. Howe, C. Knoblock, I. D. McDermott, A. Ram, M. Veloso, D. Weld, D. W.SRI, A. Barrett, D. Christianson et al., “Pddl— the planning domain definition language,” Technical Report, Tech. Rep., 1998</p>

                        <p align="justify" width="20%">[2] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot planners: Extracting actionable knowledge for embodied agents,” Thirty-ninth International Conference on Machine Learning, 2022</p>
                    </div>
                    <!-- <center>
                        <h1>Acknowledgements</h1>
                    </center> -->
                    <!-- The webpage template was borrowed from some <a href="https://nvlabs.github.io/SPADE/">GAN folks</a>. -->
                    <!-- <div style="font-size:30px">
                        <p align="justify">
                            This work has taken place in the Autonomous Intelligent Robotics (AIR) Group at SUNY Binghamton. AIR research is supported in part by grants from the National Science Foundation (IIS-1925044 and REU Supplement), Ford Motor Company (URP Awards), OPPO (Faculty Research Award), and SUNY Research Foundation
                        </p>
                    </div> -->
                    <br><br>

</html>
